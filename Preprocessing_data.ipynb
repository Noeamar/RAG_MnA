{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier CSV a été réorganisé et enregistré sous : C:\\Users\\namar\\Documents\\poc_RAG\\Projet_test\\RAG_MnA\\Data\\scraped_companies_reorganized.csv\n"
     ]
    }
   ],
   "source": [
    "# Chemin du fichier d'entrée\n",
    "input_file_path = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_companies_all_columns.csv\"\n",
    "\n",
    "# Chemin du fichier de sortie\n",
    "output_file_path = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_companies_reorganized.csv\"\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Renommer les colonnes\n",
    "df.columns = [\n",
    "    \"Logo\", \"Column_to_remove\", \"Société\", \"Pays Siege\",\n",
    "    \"Siège répertorié sur Arx\", \"Site internet\", \"Secteur\",\n",
    "    \"Mots-clés\", \"Description\", \"Dernier CA (M)\", \"Périmètre CA\",\n",
    "    \"Column_to_remove_2\", \"TCAM (%)\", \"Dirigeants/Equipe CF\", \"Cotation\", \"ISIN\"\n",
    "]\n",
    "\n",
    "\n",
    "# Sélectionner et réorganiser les colonnes selon la spécification\n",
    "columns_to_keep = [\n",
    "    \"Société\",\n",
    "    \"Pays Siege\",\n",
    "    \"Siège répertorié sur Arx\",\n",
    "    \"Site internet\",\n",
    "    \"Secteur\",\n",
    "    \"Mots-clés\",\n",
    "    \"Description\",\n",
    "    \"Dernier CA (M)\",\n",
    "    \"Périmètre CA\",\n",
    "    \"TCAM (%)\",\n",
    "    \"Dirigeants/Equipe CF\",\n",
    "    \"Cotation\",\n",
    "    \"ISIN\"\n",
    "]\n",
    "\n",
    "# Renommer les colonnes sélectionnées pour plus de clarté (facultatif)\n",
    "renamed_columns = [\n",
    "    \"Company\", \"Country Headquarters\", \"Arx Listed HQ\", \"Website\",\n",
    "    \"Sector\", \"Keywords\", \"Description\", \"Latest Revenue (M)\",\n",
    "    \"Revenue Scope\", \"CAGR (%)\", \"Executives/Team CF\", \"Rating\", \"ISIN\"\n",
    "]\n",
    "\n",
    "# Réorganiser et renommer les colonnes\n",
    "df_reorganized = df[columns_to_keep]\n",
    "df_reorganized.columns = renamed_columns\n",
    "\n",
    "# Enregistrer le fichier CSV modifié\n",
    "df_reorganized.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Le fichier CSV a été réorganisé et enregistré sous : {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de documents : 3\n",
      "Document : scraped_companies_reorganized.csv, Métadonnées : {'file_name': 'scraped_companies_reorganized.csv'}\n",
      "Document : scraped_news_grid.csv, Métadonnées : {'file_name': 'scraped_news_grid.csv'}\n",
      "Document : exported_results.csv, Métadonnées : {'file_name': 'exported_results.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger un fichier et extraire les données avec métadonnées\n",
    "def load_files_with_metadata(file_paths, metadata_file):\n",
    "    # Charger les métadonnées\n",
    "    with open(metadata_file, 'r') as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    \n",
    "    docs = []\n",
    "    for path in file_paths:\n",
    "        file_name = path.split(\"\\\\\")[-1]  # Extraire le nom du fichier\n",
    "        file_metadata = metadata.get(file_name, {})  # Récupérer les métadonnées\n",
    "\n",
    "        # Initialiser le contenu du document\n",
    "        content = \"\"\n",
    "        if path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(path)\n",
    "            content = \"\\n\".join([doc.page_content for doc in loader.load()])\n",
    "        elif path.endswith(\".txt\"):\n",
    "            loader = TextLoader(path)\n",
    "            content = \"\\n\".join([doc.page_content for doc in loader.load()])\n",
    "        elif path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path)\n",
    "            if 'Title' in df.columns and 'Content' in df.columns:\n",
    "                df['text'] = df['Title'] + \"\\n\\n\" + df['Content']\n",
    "            else:\n",
    "                df['text'] = df.apply(lambda row: ' '.join(map(str, row.values)), axis=1)\n",
    "            content = \"\\n\".join(df['text'].tolist())\n",
    "        \n",
    "        # Créer un document global pour le fichier avec les métadonnées\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"file_name\": file_name,\n",
    "                **file_metadata  # Ajouter les métadonnées depuis le JSON\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Exemple d'utilisation\n",
    "file_paths = [\n",
    "    \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_companies_reorganized.csv\",\n",
    "    \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_news_grid.csv\",\n",
    "    \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\exported_results.csv\"\n",
    "]\n",
    "\n",
    "metadata_file = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\metadata_arx.json\"\n",
    "\n",
    "# Charger les fichiers avec métadonnées\n",
    "docs = load_files_with_metadata(file_paths, metadata_file)\n",
    "\n",
    "# Résumé des documents chargés\n",
    "print(f\"Nombre total de documents : {len(docs)}\")\n",
    "for doc in docs:\n",
    "    print(f\"Document : {doc.metadata['file_name']}, Métadonnées : {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents sauvegardés dans C:\\Users\\namar\\Documents\\poc_RAG\\Projet_test\\RAG_MnA\\docs.pkl.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Exemple : Fonction pour sauvegarder les documents dans un fichier pickle\n",
    "def save_docs(docs, output_file):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(docs, f)\n",
    "\n",
    "# Exemple d'utilisation après avoir chargé les documents\n",
    "docs = load_files_with_metadata(file_paths, metadata_file)\n",
    "output_file = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\docs.pkl\"\n",
    "save_docs(docs, output_file)\n",
    "\n",
    "print(f\"Documents sauvegardés dans {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
