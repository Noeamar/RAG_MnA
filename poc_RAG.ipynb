{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remodel du fichier CSV Scraped Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier CSV a été réorganisé et enregistré sous : C:\\Users\\namar\\Documents\\poc_RAG\\Projet_test\\RAG_M-A\\Data\\scraped_companies_reorganized.csv\n"
     ]
    }
   ],
   "source": [
    "# # Chemin du fichier d'entrée\n",
    "# input_file_path = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_M-A\\\\Data\\\\scraped_companies_all_columns.csv\"\n",
    "\n",
    "# # Chemin du fichier de sortie\n",
    "# output_file_path = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_M-A\\\\Data\\\\scraped_companies_reorganized.csv\"\n",
    "\n",
    "# # Charger le fichier CSV\n",
    "# df = pd.read_csv(input_file_path)\n",
    "\n",
    "# # Renommer les colonnes\n",
    "# df.columns = [\n",
    "#     \"Logo\", \"Column_to_remove\", \"Société\", \"Pays Siege\",\n",
    "#     \"Siège répertorié sur Arx\", \"Site internet\", \"Secteur\",\n",
    "#     \"Mots-clés\", \"Description\", \"Dernier CA (M)\", \"Périmètre CA\",\n",
    "#     \"Column_to_remove_2\", \"TCAM (%)\", \"Dirigeants/Equipe CF\", \"Cotation\", \"ISIN\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Sélectionner et réorganiser les colonnes selon la spécification\n",
    "# columns_to_keep = [\n",
    "#     \"Société\",\n",
    "#     \"Pays Siege\",\n",
    "#     \"Siège répertorié sur Arx\",\n",
    "#     \"Site internet\",\n",
    "#     \"Secteur\",\n",
    "#     \"Mots-clés\",\n",
    "#     \"Description\",\n",
    "#     \"Dernier CA (M)\",\n",
    "#     \"Périmètre CA\",\n",
    "#     \"TCAM (%)\",\n",
    "#     \"Dirigeants/Equipe CF\",\n",
    "#     \"Cotation\",\n",
    "#     \"ISIN\"\n",
    "# ]\n",
    "\n",
    "# # Renommer les colonnes sélectionnées pour plus de clarté (facultatif)\n",
    "# renamed_columns = [\n",
    "#     \"Company\", \"Country Headquarters\", \"Arx Listed HQ\", \"Website\",\n",
    "#     \"Sector\", \"Keywords\", \"Description\", \"Latest Revenue (M)\",\n",
    "#     \"Revenue Scope\", \"CAGR (%)\", \"Executives/Team CF\", \"Rating\", \"ISIN\"\n",
    "# ]\n",
    "\n",
    "# # Réorganiser et renommer les colonnes\n",
    "# df_reorganized = df[columns_to_keep]\n",
    "# df_reorganized.columns = renamed_columns\n",
    "\n",
    "# # Enregistrer le fichier CSV modifié\n",
    "# df_reorganized.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(f\"Le fichier CSV a été réorganisé et enregistré sous : {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des fichiers dans docs pour préparer l'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de documents : 2\n",
      "Document : scraped_news_grid.csv, Métadonnées : {'file_name': 'scraped_news_grid.csv'}\n",
      "Document : exported_results.csv, Métadonnées : {'file_name': 'exported_results.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger un fichier et extraire les données avec métadonnées\n",
    "def load_files_with_metadata(file_paths, metadata_file):\n",
    "    # Charger les métadonnées\n",
    "    with open(metadata_file, 'r') as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    \n",
    "    docs = []\n",
    "    for path in file_paths:\n",
    "        file_name = path.split(\"\\\\\")[-1]  # Extraire le nom du fichier\n",
    "        file_metadata = metadata.get(file_name, {})  # Récupérer les métadonnées\n",
    "\n",
    "        # Initialiser le contenu du document\n",
    "        content = \"\"\n",
    "        if path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(path)\n",
    "            content = \"\\n\".join([doc.page_content for doc in loader.load()])\n",
    "        elif path.endswith(\".txt\"):\n",
    "            loader = TextLoader(path)\n",
    "            content = \"\\n\".join([doc.page_content for doc in loader.load()])\n",
    "        elif path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path)\n",
    "            if 'Title' in df.columns and 'Content' in df.columns:\n",
    "                df['text'] = df['Title'] + \"\\n\\n\" + df['Content']\n",
    "            else:\n",
    "                df['text'] = df.apply(lambda row: ' '.join(map(str, row.values)), axis=1)\n",
    "            content = \"\\n\".join(df['text'].tolist())\n",
    "        \n",
    "        # Créer un document global pour le fichier avec les métadonnées\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"file_name\": file_name,\n",
    "                **file_metadata  # Ajouter les métadonnées depuis le JSON\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Exemple d'utilisation\n",
    "file_paths = [\n",
    "    #\"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_companies_reorganized.csv\",\n",
    "    \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\scraped_news_grid.csv\",\n",
    "    \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\exported_results.csv\"\n",
    "]\n",
    "\n",
    "metadata_file = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\metadata_arx.json\"\n",
    "\n",
    "# Charger les fichiers avec métadonnées\n",
    "docs = load_files_with_metadata(file_paths, metadata_file)\n",
    "\n",
    "# Résumé des documents chargés\n",
    "print(f\"Nombre total de documents : {len(docs)}\")\n",
    "for doc in docs:\n",
    "    print(f\"Document : {doc.metadata['file_name']}, Métadonnées : {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding (Attention a ne pas le lancer a chaque fois !!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings générés et stockés dans ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# Diviser chaque document en morceaux plus petits\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Générer et persister les embeddings\n",
    "persist_directory = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\ChromaDB_2\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(\"Embeddings générés et stockés dans ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\namar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:55\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Créer les embeddings et le vectorstore FAISS\u001b[39;00m\n\u001b[0;32m      6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m----> 7\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Sauvegarder l'index FAISS dans un dossier local\u001b[39;00m\n\u001b[0;32m     10\u001b[0m faiss_index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnamar\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpoc_RAG\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProjet_test\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mRAG_MnA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mFAISS_index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\namar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\namar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1042\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m-> 1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\namar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:994\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 994\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    996\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\namar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:57\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "# Diviser chaque document en morceaux plus petits\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Créer les embeddings et le vectorstore FAISS\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embedding)\n",
    "\n",
    "# Sauvegarder l'index FAISS dans un dossier local\n",
    "faiss_index_path = \"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_MnA\\\\Data\\\\FAISS_index\"\n",
    "vectorstore.save_local(faiss_index_path)\n",
    "\n",
    "print(\"Embeddings générés et stockés dans FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namar\\AppData\\Local\\Temp\\ipykernel_30948\\622917514.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore chargé depuis le disque.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namar\\AppData\\Local\\Temp\\ipykernel_30948\\622917514.py:21: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats de la requête :\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Imagerie Cardinet France Paris www.imagerie-cardinet.fr Santé Intégration de logiciel (radiologie), Imagerie médicale, Image numérique, Imagerie dentaire, Traitement numérique d'images, Radiologie, Analyse médicale, Traitement d'image numérique Imagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients. n.a. n.a. n.a. n.a. n.a. n.a.\n",
      "Build-up : Imagerie Cardinet se rapproche du groupe de centres de radiologie générale Radiologie Paris Ouest Soutenue par Andera Acto depuis 2023, Imagerie Cardinet (Île-de-France / FRA), entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients, se rapproche de Radiologie Paris Ouest (Île-de-France / FRA), groupe de centres de radiologie générale et spécialisée. Cette collaboration vise à renforcer leur modèle entrepreneurial en\n",
      "Cession : Imagerie Cardinet se rapproche du groupe de centres de radiologie générale Radiologie Paris Ouest Soutenue par Andera Acto depuis 2023, Imagerie Cardinet (Île-de-France / FRA), entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients, se rapproche de Radiologie Paris Ouest (Île-de-France / FRA), groupe de centres de radiologie générale et spécialisée. Cette collaboration vise à renforcer leur modèle entrepreneurial en\n",
      "Build-up : Imagerie Cardinet se rapproche du groupe de centres de radiologie générale Radiologie Paris Ouest Soutenue par Andera Acto depuis 2023, Imagerie Cardinet (Île-de-France / FRA), entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients, se rapproche de Radiologie Paris Ouest (Île-de-France / FRA), groupe de centres de radiologie générale et spécialisée. Cette collaboration vise à renforcer leur modèle entrepreneurial en imagerie médicale et à accélérer le développement de leurs activités dans la région parisienne. Dans le cadre de la transaction, Azulis Capital a cédé sa participation minoritaire détenue depuis 2019 dans\n",
      "Cession : Imagerie Cardinet se rapproche du groupe de centres de radiologie générale Radiologie Paris Ouest Soutenue par Andera Acto depuis 2023, Imagerie Cardinet (Île-de-France / FRA), entreprise spécialisée dans les services d'imagerie médicale pour les prestataires de soins de santé et les patients, se rapproche de Radiologie Paris Ouest (Île-de-France / FRA), groupe de centres de radiologie générale et spécialisée. Cette collaboration vise à renforcer leur modèle entrepreneurial en imagerie médicale et à accélérer le développement de leurs activités dans la région parisienne. Dans le cadre de la transaction, Azulis Capital a cédé sa participation minoritaire détenue depuis 2019 dans Radiologie Paris Ouest (Andera Partners, communiqué de presse) 29/11/2024\n"
     ]
    }
   ],
   "source": [
    "# Recharger le vectorstore depuis le répertoire persist_directory\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"C:\\\\Users\\\\namar\\\\Documents\\\\poc_RAG\\\\Projet_test\\\\RAG_M-A\\\\Data\\\\ChromaDB\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "print(\"VectorStore chargé depuis le disque.\")\n",
    "\n",
    "\n",
    "# Créer un système de récupération\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Utiliser Maximal Marginal Relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 10,  # Récupérer plus de documents\n",
    "        \"score_threshold\": 0.01  # Réduire le seuil de score pour inclure plus de résultats\n",
    "    }\n",
    ")\n",
    "\n",
    "# Exemple de requête\n",
    "query = \"Imagerie cardinet\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\nRésultats de la requête :\")\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération avec LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG MULTI QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vulcain Engineering connaît plusieurs nouveautés significatives récemment, renforçant sa position sur le marché de l'ingénierie dans le secteur de l'énergie et au-delà :\\n\\n1. **Réorganisation du Capital via un LBO** :\\n   - **Opération de LBO** : En 2022, Vulcain Engineering a procédé à une réorganisation de son capital à l'occasion d'un leveraged buy-out (LBO).\\n   - **Nouveau Consortium d'Investisseurs** : L'opération a accueilli un consortium composé de **Ardian**, **Tikehau Capital**, **EMZ**, **Bpifrance**, **Amundi** et du **Fonds France Nucléaire** (géré par Siparex).\\n   - **Soutien Financier** : Cette réorganisation a été soutenue par une dette senior provenant d'un pool bancaire et par un financement mezzanine de **Eurazeo Private Debt**.\\n   - **Cession des Participations** : Les actionnaires existants, **Equistone Partners Europe** et **Sagard**, ont cédé leurs participations dans ce cadre.\\n\\n2. **Stratégie de Build-up et Acquisitions** :\\n   - **Acquisition d'Evolutec Ingénierie** : En 2019, Vulcain Engineering a racheté Evolutec Ingénierie, renforçant ainsi son expertise dans les domaines de l'énergie et de la chimie.\\n   - **Acquisition d'Apsalys** : Plus récemment, Vulcain a acquis **Apsalys**, un éditeur de logiciels de gestion de la qualité basés sur le cloud pour le secteur des sciences de la vie. Cette acquisition permet à Vulcain de diversifier son offre et de prioriser son développement dans ce secteur en pleine croissance.\\n   - **Acquisition de Pagoline Groupe** : Vulcain Engineering a également acquis **Pagoline Groupe**, spécialisé dans l'ingénierie appliquée aux réseaux et infrastructures, notamment dans l'énergie, le transport ferroviaire et les télécommunications. Cette acquisition est un élément clé pour la croissance à long terme de l'entreprise.\\n\\n3. **Expansion et Renforcement de l'Offre de Services** :\\n   - **Expansion Internationale** : La nouvelle structure capitalistique vise à soutenir l'expansion internationale de Vulcain Engineering, lui permettant de renforcer sa présence sur de nouveaux marchés.\\n   - **Diversification des Services** : En intégrant des entreprises comme Apsalys et Pagoline, Vulcain renforce son offre de services, couvrant désormais davantage de secteurs tels que les sciences de la vie, les infrastructures de transport et les télécommunications.\\n\\n4. **Soutien de Partenaires Stratégiques** :\\n   - **Présence dans les Portefeuilles d'Investisseurs** : Vulcain est également présente dans les portefeuilles de **Nixen Partners**, **Trocadero Capital Partners** et **Initiative & Finance**, ce qui témoigne de la confiance continue des investisseurs dans la stratégie de croissance de l'entreprise.\\n\\nCes développements témoignent de la dynamique de croissance de Vulcain Engineering, qui s'appuie sur des opérations de financement structurées et une stratégie d'acquisitions ciblées pour renforcer sa position sur le marché et diversifier ses domaines d'expertise.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate ten \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(model= 'o1-mini') \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    " \n",
    "# Retrieve\n",
    "question = \"Quelles nouveautées pour Vulcain ingénierie ?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Tu es un assistant chatbot qui travaille dans un cabinet de finance d'entreprise. Ton rôle est de donner les informations les plus pertinentes possibles en te basant sur les sources que tu as. Voici le contexte pour t'aider:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model='o1-mini')\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Fiche Société : Imagerie Cardinet**\\n\\n---\\n\\n**Nom de la Société :**  \\nImagerie Cardinet\\n\\n**Localisation :**  \\nParis, France\\n\\n**Site Web :**  \\n[www.imagerie-cardinet.fr](http://www.imagerie-cardinet.fr)\\n\\n**Secteur d'Activité :**  \\nSanté – Imagerie Médicale\\n\\n**Domaines de Compétence :**  \\n- Intégration de logiciels de radiologie  \\n- Imagerie médicale  \\n- Imagerie dentaire  \\n- Image numérique  \\n- Traitement numérique d'images  \\n- Analyse médicale  \\n- Traitement d'image numérique\\n\\n**Description :**  \\nImagerie Cardinet est une entreprise spécialisée dans les services d'imagerie médicale destinés aux prestataires de soins de santé et aux patients. Elle offre une gamme complète de solutions numériques et logicielles pour améliorer la qualité et l'efficacité des diagnostics médicaux.\\n\\n**Principales Activités :**  \\n- Développement et intégration de logiciels de radiologie  \\n- Fourniture de services d'imagerie médicale et dentaire  \\n- Traitement et analyse numérique des images médicales  \\n- Support et maintenance des systèmes d'imagerie pour les établissements de santé\\n\\n**Dernières Actualités :**  \\n- **Collaboration avec Radiologie Paris Ouest (2023) :**  \\n  Imagerie Cardinet a établi un partenariat avec Radiologie Paris Ouest, un groupe de centres de radiologie générale et spécialisée. Cette collaboration, soutenue par Andera Acto depuis 2023, vise à renforcer le modèle entrepreneurial des deux entités et à accélérer le développement de leurs activités dans la région parisienne.\\n\\n- **Transaction avec Azulis Capital (Depuis 2019) :**  \\n  Dans le cadre de cette transaction, Azulis Capital a cédé sa participation minoritaire détenue depuis 2019, permettant à Imagerie Cardinet de poursuivre son plan de croissance et d'innovation dans le domaine de l'imagerie médicale.\\n\\n**Stratégie de Développement :**  \\nImagerie Cardinet ambitionne de renforcer sa position sur le marché de l'imagerie médicale en France en diversifiant son offre et en élargissant son portefeuille clients. La société mise également sur des partenariats stratégiques et des collaborations pour accélérer son expansion et intégrer de nouvelles technologies dans ses services.\\n\\n**Informations Financières :**  \\nLes informations financières spécifiques telles que le chiffre d'affaires ne sont pas fournies dans le contexte disponible.\\n\\n**Contacts Clés :**  \\nLes informations concernant les dirigeants ou les contacts spécifiques ne sont pas disponibles dans le contexte fourni.\\n\\n---\\n\\n**Résumé :**  \\nImagerie Cardinet se positionne comme un acteur clé dans le secteur de l'imagerie médicale en France, offrant des solutions innovantes et intégrées aux professionnels de santé et aux patients. Grâce à des partenariats stratégiques et une expertise technologique, l'entreprise vise à améliorer la qualité des diagnostics médicaux et à soutenir la croissance continue de ses activités.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "question = \"Fais moi une fiche société sur Cardinet ?\"\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(model='o1-mini')\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "llm = ChatOpenAI(model='o1-mini')\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
